# The Human Element: The Ethics of AI in Defense

_“The ‘felt’ element in troops, not expressible in figures, had to be guessed at by the equivalent of Plato’s δόξά [judgement], and the greatest commander of men was he whose intuitions most nearly happened. Nine-tenths of tactics were certain enough to be teachable in schools; but the irrational tenth was like the kingfisher flashing across the pool, and in it lay the test of generals. It can only be ensued by instinct, sharpened by thought practising the stroke so often that at the crisis it is as natural as a reflex.”_
<p align=right>-T.E. Lawrence</p>

Lawrence penned those words in his journal during the height of the Arab Revolt of the early 20th century. He was serving as the liaison between the Arab and European worlds as the former sought to break free from its master - the Ottoman Empire. The Empire, on paper, was considerably more formidable than the tenuously allied tribes of the Middle East. It had far more resources at its disposal, far more troops, and, chiefly, was far more advanced; an elaborate network of railroads across the region was its most prominent display of this fact. Lawrence, practically speaking, had only men and animals to work with. Yet, in the most dramatic display of man versus machine the world had ever seen, man won. 

The Arab Revolt was yet another example of what all successful military figures have known since the inception of warfare itself: war is humanity’s game; for all the technology in the world, a war cannot be won without soldiers. The statement seems obvious - too obvious to even note - but its veracity is fading. Technology is advancing at a fierce pace, and recent developments in AI and its applications in war have called this axiom into question. The future of warfare might very well not involve human soldiers at all.

The use of drones, precision-guided systems, and artificial intelligence algorithms have simultaneously brought about optimism over their potential to save lives and concern over their potential to take them. We find ourselves at a rather interesting point in the trajectory of these systems: they have yet to be deployed in a fully autonomous role, that is, a human is still required to give the final command to attack. As such, we have a great deal of control in setting the precedent and guidelines for the use (or ban) of autonomous weapons in warfare.

Exactly what to do, however, is controversial. There are two main schools of thought on the matter - one which believes that these weapons are more capable of operating ethically due to their superhuman abilities, and one which believes these weapons are less capable because they are not human at all. In this paper, we will traverse the history of autonomous weapons and their current use cases before weighing potential concerns and benefits in the context of ethics as we explore further whether autonomous weapons and systems should be allowed in war. The answer, we find, is that autonomous weapons ought to be allowed in war, but with extremely close human oversight. Man is emboldened in his ability to do good by these machines, and in turn these machines are hindered in their ability to do evil by man. The human aspect of war is just too important to remove entirely.

## An Ethical Foundation of Technology in War
_“War must be, while we defend our lives against a destroyer who would devour all; but I do not love the bright sword for its sharpness, nor the arrow for its swiftness, nor the warrior for his glory. I love only that which they defend.”_
<p align=right>-J.R.R. Tolkien</p>

At the heart of this issue lies the central ideas of justice and just war. Autonomous systems are only able to act ethically if they are operating within the confines of an ethical conflict. Plato is his Laws contemplates the “right” way to go about war. He states that neither real play nor education are found in war, which he holds to be the most important things; war itself is not an endeavor worth pursuing. _“It is the life of peace that everyone should live as much and as well as he can,”_ and as such, the only just war is one waged that there might be peace.

This notion itself produces many sub-cases and ethical dilemmas, and to address each and every one individually would be impractical. For our purposes, we will leave our ethical foundation somewhat nebulous: any foreign policy act that supports peace or the legitimate pursuit of peace is ethical. 

On 14 November, 1937, Winston Churchill gave a speech with much the same sentiment. He holds the pursuit of goodness far above the pursuit of science: “It is above all essential that the man and woman of today should realize upon how much lower a plane science stands than that of manners and morals… Freedom is worth far more than electricity.” Yet technological progress is seemingly unstoppable, and for all its great benefits, it also brings the potential for destruction on an apocalyptic scale. It is mankind’s supreme task, then, to direct the course of this progress in a virtuous manner, or at least attempt to. _“It may not be in our power to decide the immediate future of the world, but it is our right and duty to choose - and to choose well.”_

So we ought to strive for peace, and in that striving for peace we must combat our rapid pace of progress before it becomes our undoing. Directing the course of progress, then, towards peaceful end states is the highest ethical pursuit of technologists in defense. 

The most obvious conclusion from this is that we ought to not pursue weaponry at all - a world without advanced weaponry is surely the most peaceful one. Ideally, this would certainly hold true, but at present we live in a far more complex world; one where morals are not the highest pursuit of all mankind, and where organized bands of individuals can deal significant damage to states and their citizens. _“There is no reason why a base, degenerate, immoral race should not make an enemy far above them… simply because they… [possessed] some new death-dealing or terror-working process and were ruthless in its employment.”_

If such a people were to develop an autonomous weapons system and employ it to their own nefarious ends, it would certainly be the most ethical course of action to develop our own to stop them. But what if they can exercise their force faster than we can respond? Churchill himself plays with this idea - _“shall we have time?”_ It seems, thus, that in balancing realism and ethics, the right course is to - at the very least - develop capable systems of our own, that we might deter its use across the globe.

As we rise to meet our “supreme task,” we must then ourselves be ethical in the process. We must outline an ethical standard by which to evaluate autonomous systems, and choose not to deploy them if they fail to meet this standard. This standard is comprised of the following parts: functionality, transparency, capacity for good… and evil.

**Functionality**

The first standard by which we must evaluate any AI-driven weapons system is how well-defined it is. We must ask ourselves what the expected behavior is in a wide variety of scenarios and situations, and then verify that the actual behavior is in alignment with these expectations. Simply put, the system must work, it must be as bug-free as possible, and it must function with well-defined intent. When the software fails, does it fail ethically?

**Transparency**

It is easy for artificial intelligence algorithms to become a black box of sorts, where the behavior of the algorithm is not straightforward to understand. We can only evaluate the ethicality of decisions by tracing the decision-making process to its root. There needs to be a clear means of assessing the decisions of the algorithms that drive the systems, and there needs to be a clear definition of who is accountable for the decision it makes to make it ethical. This also applies to the training process, there ought to be transparency and serious thought given to the way the algorithms are trained. The transparency in the process further allows us to correct and reevaluate it if flaws are discovered.

**Capacity for Good... and Evil**

At the end of the day, these systems are created to amplify the behaviors of mankind, both positively and negatively. As such, the last major question that needs to be evaluated before such a weapon is deployed would be, “Is goodness maximized when this system is in virtuous hands? And is evil minimized when this system is in malicious hands?” Defining “good” and “evil” is a philosophical struggle that has been ongoing since the dawn of Man. Therefore, we will pull from Kantian ethics to create a consistent paradigm to answer this question. In his Groundwork of the Metaphysics of Morals, Kant writes his first Categorical Imperative: _“Act only according to that maxim by which you can at the same time will that it should become a universal law.”_ In the context of war, this breaks down to the simple question - _“if every other soldier acted in this way, would society progress?”_
	
When all is said and done, so long as the weapon works, the dominant factor in determining the weapon’s ethicality is its capacity for good and evil. Truly autonomous systems are merely an extreme extension of the humans wielding them, and as such only are as capably good or evil as they are. With this framework in place, we can begin to look at autonomous systems in practice and their potential place in defense.





