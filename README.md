# The Human Element: The Ethics of AI in Defense

_“The ‘felt’ element in troops, not expressible in figures, had to be guessed at by the equivalent of Plato’s δόξά [judgement], and the greatest commander of men was he whose intuitions most nearly happened. Nine-tenths of tactics were certain enough to be teachable in schools; but the irrational tenth was like the kingfisher flashing across the pool, and in it lay the test of generals. It can only be ensued by instinct, sharpened by thought practising the stroke so often that at the crisis it is as natural as a reflex.”_ -T.E. Lawrence

Lawrence penned those words in his journal during the height of the Arab Revolt of the early 20th century. He was serving as the liaison between the Arab and European worlds as the former sought to break free from its master - the Ottoman Empire. The Empire, on paper, was considerably more formidable than the tenuously allied tribes of the Middle East. It had far more resources at its disposal, far more troops, and, chiefly, was far more advanced; an elaborate network of railroads across the region was its most prominent display of this fact. Lawrence, practically speaking, had only men and animals to work with. Yet, in the most dramatic display of man versus machine the world had ever seen, man won. 

The Arab Revolt was yet another example of what all successful military figures have known since the inception of warfare itself: war is humanity’s game; for all the technology in the world, a war cannot be won without soldiers. The statement seems obvious - too obvious to even note - but its veracity is fading. Technology is advancing at a fierce pace, and recent developments in AI and its applications in war have called this axiom into question. The future of warfare might very well not involve human soldiers at all.

The use of drones, precision-guided systems, and artificial intelligence algorithms have simultaneously brought about optimism over their potential to save lives and concern over their potential to take them. We find ourselves at a rather interesting point in the trajectory of these systems: they have yet to be deployed in a fully autonomous role, that is, a human is still required to give the final command to attack. As such, we have a great deal of control in setting the precedent and guidelines for the use (or ban) of autonomous weapons in warfare.

Exactly what to do, however, is controversial. There are two main schools of thought on the matter - one which believes that these weapons are more capable of operating ethically due to their superhuman abilities, and one which believes these weapons are less capable because they are not human at all. In this paper, we will traverse the history of autonomous weapons and their current use cases before weighing potential concerns and benefits in the context of ethics as we explore further whether autonomous weapons and systems should be allowed in war. The answer, we find, is that autonomous weapons ought to be allowed in war, but with extremely close human oversight. Man is emboldened in his ability to do good by these machines, and in turn these machines are hindered in their ability to do evil by man. The human aspect of war is just too important to remove entirely.

## An Ethical Foundation of Technology in War
_“War must be, while we defend our lives against a destroyer who would devour all; but I do not love the bright sword for its sharpness, nor the arrow for its swiftness, nor the warrior for his glory. I love only that which they defend.”_ -J.R.R. Tolkien

At the heart of this issue lies the central ideas of justice and just war. Autonomous systems are only able to act ethically if they are operating within the confines of an ethical conflict. Plato is his Laws contemplates the “right” way to go about war. He states that neither real play nor education are found in war, which he holds to be the most important things; war itself is not an endeavor worth pursuing. _“It is the life of peace that everyone should live as much and as well as he can,”_ and as such, the only just war is one waged that there might be peace.

This notion itself produces many sub-cases and ethical dilemmas, and to address each and every one individually would be impractical. For our purposes, we will leave our ethical foundation somewhat nebulous: any foreign policy act that supports peace or the legitimate pursuit of peace is ethical. 

On 14 November, 1937, Winston Churchill gave a speech with much the same sentiment. He holds the pursuit of goodness far above the pursuit of science: “It is above all essential that the man and woman of today should realize upon how much lower a plane science stands than that of manners and morals… Freedom is worth far more than electricity.” Yet technological progress is seemingly unstoppable, and for all its great benefits, it also brings the potential for destruction on an apocalyptic scale. It is mankind’s supreme task, then, to direct the course of this progress in a virtuous manner, or at least attempt to. _“It may not be in our power to decide the immediate future of the world, but it is our right and duty to choose - and to choose well.”_

So we ought to strive for peace, and in that striving for peace we must combat our rapid pace of progress before it becomes our undoing. Directing the course of progress, then, towards peaceful end states is the highest ethical pursuit of technologists in defense. 

The most obvious conclusion from this is that we ought to not pursue weaponry at all - a world without advanced weaponry is surely the most peaceful one. Ideally, this would certainly hold true, but at present we live in a far more complex world; one where morals are not the highest pursuit of all mankind, and where organized bands of individuals can deal significant damage to states and their citizens. _“There is no reason why a base, degenerate, immoral race should not make an enemy far above them… simply because they… [possessed] some new death-dealing or terror-working process and were ruthless in its employment.”_

If such a people were to develop an autonomous weapons system and employ it to their own nefarious ends, it would certainly be the most ethical course of action to develop our own to stop them. But what if they can exercise their force faster than we can respond? Churchill himself plays with this idea - _“shall we have time?”_ It seems, thus, that in balancing realism and ethics, the right course is to - at the very least - develop capable systems of our own, that we might deter its use across the globe.

As we rise to meet our “supreme task,” we must then ourselves be ethical in the process. We must outline an ethical standard by which to evaluate autonomous systems, and choose not to deploy them if they fail to meet this standard. This standard is comprised of the following parts: functionality, transparency, capacity for good… and evil.

**Functionality**

The first standard by which we must evaluate any AI-driven weapons system is how well-defined it is. We must ask ourselves what the expected behavior is in a wide variety of scenarios and situations, and then verify that the actual behavior is in alignment with these expectations. Simply put, the system must work, it must be as bug-free as possible, and it must function with well-defined intent. When the software fails, does it fail ethically?

**Transparency**

It is easy for artificial intelligence algorithms to become a black box of sorts, where the behavior of the algorithm is not straightforward to understand. We can only evaluate the ethicality of decisions by tracing the decision-making process to its root. There needs to be a clear means of assessing the decisions of the algorithms that drive the systems, and there needs to be a clear definition of who is accountable for the decision it makes to make it ethical. This also applies to the training process, there ought to be transparency and serious thought given to the way the algorithms are trained. The transparency in the process further allows us to correct and reevaluate it if flaws are discovered.

**Capacity for Good... and Evil**

At the end of the day, these systems are created to amplify the behaviors of mankind, both positively and negatively. As such, the last major question that needs to be evaluated before such a weapon is deployed would be, “Is goodness maximized when this system is in virtuous hands? And is evil minimized when this system is in malicious hands?” Defining “good” and “evil” is a philosophical struggle that has been ongoing since the dawn of Man. Therefore, we will pull from Kantian ethics to create a consistent paradigm to answer this question. In his Groundwork of the Metaphysics of Morals, Kant writes his first Categorical Imperative: _“Act only according to that maxim by which you can at the same time will that it should become a universal law.”_ In the context of war, this breaks down to the simple question - _“if every other soldier acted in this way, would society progress?”_
	
When all is said and done, so long as the weapon works, the dominant factor in determining the weapon’s ethicality is its capacity for good and evil. Truly autonomous systems are merely an extreme extension of the humans wielding them, and as such only are as capably good or evil as they are. With this framework in place, we can begin to look at autonomous systems in practice and their potential place in defense.

## A Brief History of Autonomous Weapons
_“We're so enamored of technological advancements that we fail to think about how to best apply those technologies to what we're trying to achieve. This can mask some very important continuities in the nature of war and their implications for our responsibilities as officers.”_ -H.R. McMaster

On October 2, 1918, just one day after Lawrence’s revolt had seized the city of Damascus and effectively reclaimed their independence, the United States tested the world’s first unmanned weapons system. The Kettering Aerial Torpedo, named for its creator, Charles Kettering, was a 530 pound flying bomb that was launched off of guide rails towards its intended target. It featured a primitive in-flight guidance system that provided stability (but no directional control) to the aircraft, and a preset timing mechanism that would send its payload earthward, detonating on impact. The Torpedo, nicknamed “Bug,” would come to be known as the forerunner of present-day cruise missiles.

Less than three decades later, on August 27, 1943, the German Army successfully executed the world’s first guided missile attack, sinking the British sloop Egret with one of its brand new rockets, the Henschel Hs 293. The extensive research that the Germans had conducted on rocket systems was picked up by the Americans at the end of World War II, who would go on to develop many more autonomous weapon systems of their own. In the decade following the war, the United States created and deployed the Regulus series missiles, the first operational cruise missiles. These were then replaced by the more advanced Tomahawk missile, which took advantage of onboard computer systems and GPS to self-navigate to targets. The most recent iteration, the Hellfire, was deployed in 1984 for anti-armor use. These missiles have been largely repurposed, however, as the primary air-to-ground weapons system for precision drone strikes, which began with the deployment of the Predator drone in 1995.

The predator is described by the air force as a Tier II MALE UAS (medium-altitude, long-endurance unmanned aircraft system) capable of flying itself to a target, waiting overhead there for 14 hours, then returning to base. It is currently completely autonomous in all areas other than pulling the trigger on strikes.

In each of the aforementioned weapons systems, some level of autonomy is at play as the weapon executes or is used to execute its primary task. Modern autonomous weapons technology is a result of continuing efforts to provide electronic aids and enhancements to soldiers and operators downrange. These developments have served to offload a growing share of navigation, target acquisition, and attack functions from the human operator. Modern advances in processors, sensors, and algorithms now make possible the imminent development of completely autonomous weapons.

The United States has not been the only nation to pursue autonomy. America’s most traditional rival, Russia, had a missile program of its own all throughout the Cold War as well as its own drone program. In China, we have witnessed the largest-scale state surveillance program in history powered by artificial intelligence, among their own formidable array of weapons-capabilities. Both nations are developing AI-powered weapons systems., Even Iran and North Korea, two nations on the United States State Sponsor of Terrorism list, have growing artificial intelligence capabilities. Iran especially has an up-and-coming emerging technologies market, and is currently developing an AI-powered autonomous weapons program of its own.

Today, there are no autonomous weapons in use by the U.S. Department of Defense that require no human intervention. However, the military is actively looking into such weapons as it seeks to innovate. Artificial Intelligence is becoming a critical part of modern warfare. Compared with conventional systems, military systems equipped with AI have the potential to handle large volumes of data more efficiently, and the operation of combat systems across the board because of its inherent competencies in computation and fast decision-making. Early versions of these systems are already being employed, though none have free access to the weapons they are paired with. In all cases where a non-human is allowed to automatically fire a gun, a human must make the final judgement.

Outside of its many non-controversial (or at least substantially less controversial) uses in areas like logistics, transportation, and training, AI presently offers many powerful solutions to problems in more direct areas like cybersecurity, warfare platforms, and target recognition.

The first application of these is a relatively new domain - cybersecurity. AI is incredibly good at detecting patterns and anomalies in those patterns. As a result, it has found widespread use in cyber defense - it is simply superhuman in its skill at detecting and preventing attacks. Humanity still reigns in cyber offense; AI is not yet quite as innovative as human hackers are, though this is changing fast. Cybersecurity will be an enormous use case of AI.

More tangibly, AI is also seeing and will see widespread use in the field of target recognition. The ability of machines to rapidly identify patterns and classify objects in images and videos is unparalleled in its speed and scale. This fact alone makes it a phenomenal candidate for target recognition systems, and the Department of Defense has already begun testing and preliminary deployment of such systems in preexisting weapons and surveillance platforms. Such technologies will apply to both autonomous and semi-autonomous weapons platforms as they either act independently on the targets they find or relay them back to human operators.

## DROIDVision - A Case Study
_“Some people call this artificial intelligence, but the reality is this technology will enhance us. So instead of artificial intelligence, I think we'll augment our intelligence.”_ -Ginni Rometty

As part of an ongoing project for the SDC (Soldier Design Competition), a group of Army ROTC cadets at MIT came up with the idea for DROIDVision (Digitally-Regulated Objects of Interest Detection Vision). DROIDVision is meant to solve problems with situational awareness in military operations by providing intelligence for a given area of operation (AO) before, during, and after soldiers are there. This is the mission statement for the project:

```
Information about an AO is important to the foot-soldier because he is the one on the ground making instantaneous
decisions with immediate consequences. Information about an AO is also important to higher command because it informs
these macro-decision makers in their leadership and directive responsibilities of the foot-soldiers. DROID Vision
enables real-time, passive, and automatic dissemination of visual information that describes a particular AO.
```

At its core, DROIDVision is a tactically-scaled relay driven by AI to provide distances, directions, and descriptions of objects in certain areas, whether they be people, vehicles, animals, etc. Simply speaking, it was essentially a small Raspberry Pi computer powered by Google’s MobileNet Machine Vision architecture, and a camera. Across a number of trials that varied number of people in frame, their distance from the device, and their orientation with respect to it, DROIDVision has accuracies ranging from 50% for side-profiles of people on the low end to 90% for small groups of people at a fixed distance on the high end. The full results of the DROID prototype, as well as images of it and its components, can be found in _Appendix A_.

Compared to the kinds of technologies being developed by the United States military, the DROID project is orders of magnitude away in terms of scale, resources, and performance. But it does offer many parallels to the real system in terms of operation. We can hold DROIDVision to our standard and abstract our findings to autonomous systems _ut totum_.

First, the functionality of DROIDVision is well defined. It is meant to return distance, direction, and a basic description of the object it sees to the user. In some cases, namely the trials producing certainties around .5, the functionality of DROID falls off. However, while not particularly useful when so uncertain, DROID fails ethically: failing to alert a user to an object is the same as DROID not even being present in the first place. If DROID wrongfully identifies an object when there is none, it takes no further action. Atomically, DROID has only a net-positive impact on the intelligence process.

Transparency is where we begin to find DROID lacking, and for much the same reason that other technologies fail to meet the ethical standard. Acquiring data and training AI algorithms is highly resource-intensive, requiring far more time than our team was able to give it. As such, the team “outsourced” much of that work to Google’s MobileVisionV2 platform. The platform, while incredibly handy, offers no clear transparency into its inner workings or how it was trained. Our team would not be able to reasonably explain the intricacies of the algorithm or the decisions it makes.

DROID’s capacity for good in virtuous hands is clear - it could inform leaders as to the whereabouts of various objects across a given area of operation. Its capacity for bad is quite limited; in much the same way as in virtuous hands, DROID can do little more than inform at this stage. It itself cannot act in any nefarious ways to obtain or execute on that information.

This capacity, however, changes greatly with context, and it is here that we can begin to abstract to autonomous weapons as a whole. Imagine DROID was improved upon to the point of perfection - it was 100% accurate all the time for every object. Now imagine DROID was placed into the context of a larger decision-making process, say, for drone strikes. If DROID were indeed totally perfect in the decisions it made, allowing it to make the call for drone strikes in a given area would actually be more ethical than our current system, in that it would lead to less mistakes and less unnecessary loss-of-life in a process we already employ.

The reality of AI, however, is that the algorithms are not perfect, either in the probabilities they provide or the more meta interpretation that the rules they follow are not encompassing enough to be applied to such serious situations; we will explore this further in the handling criticisms for autonomous systems.

With a more realistic accuracy rate of 95%, we begin to run into ethical challenges in the _functionality_ and _capacity for good/evil areas_. A virtuous actor might exercise far more caution in deploying the system, or even limiting deployment exclusively to dangerous regions; a malicious actor, on the other hand, might not exercise such restraint. By deploying such a system indiscriminately, they put innocent people at risk, and even a rate as high as 95% means 5 wrong strikes for every 100. The capacity for harm skyrockets. Even with technologies that operate ethically under ideal conditions, they are constrained by the technical realities that make them far more complicated.

## An Argument for Autonomy

_“To make robots practical, flaws must be removed."_

_"To make robots endearing, flaws must be added.”_

-Khang Kijarro Nguyen

For autonomous weapons to be ethical in the abstract, they must meet our standards of _functionality_, _transparency_, and _capacity for good and evil_. If these systems can meet that standard idealistically and conceptually, we have a standard to build our machines towards. If they do not, we save ourselves decades of resources and effort that would never have amounted to anything substantially ethical. It is important to set the right precedent now, before it is set for us.

Functionally speaking, these weapons ought to mimic the behavior of a virtuous, hypereffective soldier in every way. They scrupulously abide by the rules of engagement; they never superfluously put life in harm’s way; they never make a mistake. These systems are, for all intents and purposes, a human soldier without the physical shortcomings of humanity.

Tony Cerri, a director for Data and Simulation at the U.S. Army Training and Doctrine Command, cites a civilian-dense urban environment as the shining use case for these robots: _“Imagine that we are fighting in a city and we have a foe that is using human life indiscriminately as a human shield...You can’t deal with every situation. You are going to make a mistake.”_ Robots, programmed to navigate such complex scenarios, are the best solution to this problem, according to Cerri. _“A robot, operating within milliseconds, looking at data that you can’t even begin to conceive, is going to… limit collateral damage.”_ This notion of superhuman functionality is echoed by Maj. Jeffrey S. Thurnher, U.S. Army, _“[lethal autonomous robots] have the unique potential to operate at a tempo faster than humans can possibly achieve and to lethally strike even when communications links have been severed.”_

A superhuman soldier offers superhuman results when it succeeds, and superhuman results when it fails. Ethical success can be ensured by consciously crafting ethical intent. Failure presents a far more cumbersome problem. When a human soldier begins to stray away from success, they can be ordered to stand down by a superior or checked by their peers. Similar mechanisms would need to be implemented in order to make the failure of these systems as ethical as possible.

Beyond just better performance, the use of these systems offers some far more obvious functional outcomes, as well. Every downrange autonomous weapon saves human soldiers from dangerous situations, especially more one-sided endeavors, like clearing routes of IEDs or reconnoitering hostile environments.

Examining the transparency of autonomous weapons paints a far less optimistic picture. While ethical training and design of the algorithms might paint a clear picture of how these systems make their decisions, responsibility for them is ambiguous. Many opponents believe that accountability cannot merely be transferred to a machine, and these systems cannot be authorized to operate without a human taking responsibility for its decisions. 

In modern conflicts, accountability is clear. While human error may be more tolerable in stressful combat environments, the onus of the error falls undisputedly on the human that made the mistake, and their chain of command. There have been cases of the entire chain of command being relieved over the unethical actions of soldiers at the lowest level. It is hard to nail down the accountable party in a fully autonomous system - is it the manufacturer? The software developers? The officer who authorized its use? The officer most immediately overseeing its actions on the ground? There needs to be a human in the equation, if for no other reason than to take the blame.

The capacity for good and evil is where most viewpoints diverge. Because we envision these weapons to be extreme extensions of human combatants, the humanity (or lack thereof) of these systems is the root of their capacity for virtuousness.

In _The Case for Ethical Autonomy in Unmanned Systems_, Ronald Arkin argues that autonomous weapons will actually act more “humanely” on the battlefield, namely because they do not need to be programmed with a self-preservation instinct. This alone mitigates the  “shoot first, ask questions later” attitudes that lead to morally gray outcomes. Their judgements will not be clouded by stress or fear, and their ability to process information offers far more objectivity than their human counterparts. Because machines are hardwired to follow rules, they can be relied upon to report the ethical infractions they might observe. For Arkin, the reality of war and how unethical it already is is case enough for the use of autonomous weapons - at least we can trust the algorithm to react by the rules and in the same way, every single time.

Those opposing the use of these weapons cite their judgement for precisely the opposite reason. Artificial Intelligence lacks the same sort of compassion and empathy that human combatants bring to the battlefield, and it is far less inhibited in its decision to kill. Without the capacity to exercise human judgement, these systems could not make the subjective assessments that underlie many of the protections of international law. Indeed, many of the most ethically challenging cases arise when orders and the applied rules of war conflict with our humanity and morals.

Paul Scharre, a former Special Operations Reconnaissance Team Leader, reflected on one mission that presented him with such a case. His team had been sent to the Afghanistan-Pakistan border to look for Taliban fighters, and there they encountered a little girl. Though initially seemingly innocent, his team soon realized that she was, in fact, reporting information about his team back to local Taliban officials. By the rules of war, they were legally cleared to engage - but they didn’t. Scharre said shooting the child was not an option, it simply would not have been the right thing to do.

Scharre believes the outcome would have been different were a robot in his place, and raised a question about the conflict between doing what is legal and doing what is right: _“How would you design a robot to know [that difference]? And how would you even begin to write down those rules ahead of time? What if you didn’t have a human there, to interpret these, to bring that whole set of human values to those decisions?”_

As with many other situations, it seems that machines outperform humans in some roles, and humans outperform machines in others. In the case of defense, these roles are as rule-followers, and rule-breakers, respectively.

## The Lesser Evil and the Greater Good
_“Now I am become Death, the destroyer of Worlds.”_ -J. Robert Oppenhiemer

When presented with the monumental choice of what to do with the atomic bomb, President Harry S. Truman ultimately decided to use it. In so doing he changed the dynamics of foreign policy and warfare forever. Truman made his decision not out of malevolence, but out of growing concern for unnecessary loss of life. When faced with an amphibious invasion of Japan or the deployment of an atomic weapon, Truman chose the path of least casualties. And, as far as we can tell, he chose correctly for that metric.

We live in a world driven by the consequences of his decisions. Nuclear weapons placed an even greater emphasis on the destructive nature of war; world leaders no longer had to gamble with the fate of just their nations, but of the entire species as well. Hot conflicts turned cold and “wars” became “proxy wars.” Military struggle persisted under the hood, but with a very different face and radically different outcomes. In many ways, the world became more complex, and in many ways still more, importantly, the world became more peaceful.

Not only did these powerfully destructive weapons deter global powers from fighting directly, we have witnessed their effect as an equalizer in foreign policy as well. States like the Democratic People’s Republic of Korea and the Islamic Republic of Iran have received much attention over their own nuclear programs. Even in the case of North Korea, with their economy that is 1/500th the size of the United States, they require delicate treatment from the United States. Among other foreign policy considerations, their potential for destruction awards them far more consideration from greater powers than they would have received before such weapons existed.

There is little reason to believe the world would respond any differently to autonomous weapons. Already we see the world transforming around us in response to them, despite an ongoing ethical debate surrounding their use. AI and the weaponry it enables will be used as deterrents in foreign policy, the face of warfare will shift once again (likely to cyberspace, where AI-driven algorithms will battle it out in cyber attacks and defenses). So long as humanity uses these tools to enhance the human condition in war rather than obsolete it, these weapons will not reverse the peaceful trends of recent times.

## A Human Element
_“To do no evil is good, to intend none better.”_ -Claudius

Autonomous weapons will irreversibly change the face of war, that much is certain. Their inevitability ensures their universal development - robust arsenals are the surest guarantees of peace. How we choose to develop them here, however, is yet unclear.

Weapons that do not require a human, in their most fundamental form, have been around for millenia: bombs, mines, even the arrow after it has been loosed from the bow. Humanity has been tolerant of such weapons. In every case, the weapon, even after leaving human control, was constrained by forces we deemed to be acceptably ethical; human combatants were responsible for every input into the well defined function that was the laws of physics, and the outcome was deterministic. These new weapons are different. They make their own inputs and are their own function - their outcomes are more uncertain to human observers.

Functionally speaking, these robots of tomorrow are better than human soldiers in every way - under the pure constraint of the rules of engagement, they do exactly what they are instructed to do. This, their greatest strength, is also their most damning weakness. They do not override when things are not _right_ - they lack allegiance to an unknown, unseen force that drives humanity to do and be good. They have no honor.

Honor is one of the most fundamental attributes of the US Army officer corps. The concept has existed in some form or another since the inception of organized society; it was codified in the feudalistic structure of the medieval ages; it persists to this day in the United States Code of Military Justice:
```
...any commissioned officer, cadet or midshipman who is convicted of conduct unbecoming
an officer and a gentleman shall be punished as a court-martial may direct.
```
Honor cannot be expressed wholly in words or broken down into pure logic and symbols to be converted into an algorithm, it must simply be felt. Honor lies deep within Lawrence’s _irrational tenth_ that cannot be completely taught in schools.

For while a robot can be taught to identify enemies and innocents according to the rules of engagement better than a human and to shoot better than a human, it can never be taught that there are some moments when you _should just not shoot_ (see Appendix B for a powerful example of one such time from Lawrence’s diary). So long as honor exists in this world, autonomous systems cannot be permitted to roam the battlefield truly autonomously. There must always be a human in the loop, watching closely over these robots and taking responsibility for their actions.

So autonomous weapons ought to be developed. They ought to be used and improved upon _ad infinitum_. They will transform the world - for the better, if ethical people develop them first - and they will save many innocent lives with their superhuman fighting ability. But they will not do so completely autonomously. By implementing a tight human-AI partnership on the battlefield, we will find more ethical and more effective soldiers than ever before. Humanity cannot ever be replaced in war. It is, after all, its most essential ingredient.

-------------------
## Appendix A: DROIDVision Results and Images
_Values are certainties on scale from 0 (least certain) to 1 (most certain)_

**Number of People in Frame**
|        | 1    | 2    | 3    | 4    | 5    |
|--------|------|------|------|------|------|
| Trial1 | 0.86 | 0.85 | 0.86 | 0.83 | 0.87 |
| Trial2 | 0.88 | 0.9  | 0.82 | 0.89 | 0.84 |
| Trial3 | 0.84 | 0.86 | 0.88 | 0.85 | 0.84 |

**Distance from Camera**
|        | 0.5 m | 1 m  | 2 m  | 3 m  | 5 m  |
|--------|-------|------|------|------|------|
| Trial1 | 0.87  | 0.76 | 0.71 | 0.65 | 0.55 |
| Trial2 | 0.88  | 0.78 | 0.69 | 0.64 | 0.61 |
| Trial3 | 0.84  | 0.78 | 0.72 | 0.62 | 0.58 |

**Orientation of person (in degrees of rotation from face-forward)**
|        | 0    | 30   | 45   | 90   | 180  |
|--------|------|------|------|------|------|
| Trial1 | 0.86 | 0.71 | 0.76 | 0.55 | 0.61 |
| Trial2 | 0.88 | 0.69 | 0.71 | 0.68 | 0.53 |
| Trial3 | 0.87 | 0.75 | 0.68 | 0.52 | 0.54 |

***[Components Redacted]***

## Appendix B: Seven Pillars of Wisdom Passage

_As I rode up the bank my camel's feet scrambled in the loose ballast, and out of the long shadow of a culvert to my left, where, no doubt, he had slept all day, rose a Turkish soldier. He glanced wildly at me and at the pistol in my hand, and then with sadness at his rifle against the abutment, yards beyond. He was a young man; stout, but sulky-looking. I stared at him, and said, softly, 'God is merciful'. He knew the sound and sense of the Arabic phrase, and raised his eyes like a flash to mine, while his heavy sleep-ridden face began slowly to change into incredulous joy._

_However, he said not a word. I pressed my camel's hairy shoulder with my foot, she picked her delicate stride across the metals and down the further slope, and the little Turk was man enough not to shoot me in the back, as I rode away, feeling warm towards him, as ever towards a life one has saved. At a safe distance I glanced back. He put thumb to nose, and twinkled his fingers at me._

[Seven Pillars of Wisdom by T.E. Lawrence](http://gutenberg.net.au/ebooks01/0100111h.html)

## Work Cited

“8 Key Military Applications for Artificial Intelligence in 2018.” Accessed December 9, 2019. https://blog.marketresearch.com/8-key-military-applications-for-artificial-intelligence-in-2018.

“A Brief History of Precision Guided Weapons.” Accessed December 9, 2019. http://www.tfcbooks.com/special/missiles.htm.

“AI.” Accessed October 15, 2019. https://innovation.defense.gov/ai/.

The Strategist. “AI and National Security: Lethal Robots or Better Logistics?,” July 19, 2018. https://www.aspistrategist.org.au/ai-and-national-security-lethal-robots-or-better-logistics/.

www.army.mil. “AI Task Force Taking Giant Leaps Forward.” Accessed October 15, 2019. https://www.army.mil/article/225642/ai_task_force_taking_giant_leaps_forward.

Allen, Greg, and Taniel Chan. “Artificial Intelligence and National Security.” National Security, 2017, 132.

“Are Killer Robots the Future of War? Parsing the Facts on Autonomous Weapons - The New York Times.” Accessed December 9, 2019. https://www.nytimes.com/2018/11/15/magazine/autonomous-robots-weapons.html.

www.army.mil. “Battlefield Artificial Intelligence Gets $72M Army Investment.” Accessed October 15, 2019. https://www.army.mil/article/218354/battlefield_artificial_intelligence_gets_72m_army_investment.

“Bennett - Groundwork for the Metaphysic of Morals.Pdf.” Accessed December 9, 2019. https://www.earlymoderntexts.com/assets/pdfs/kant1785.pdf.

Bennett, Jonathan. “Groundwork for the Metaphysic of Morals.” Immanuel Kant, n.d., 53.

“‘Black Hearts’ Case Study: The Yusufiyah Crimes, Iraq, March 12, 2006 | Written Case Study | CAPL.” Accessed December 9, 2019. https://capl.army.mil/case-studies/wcs-single.php?id=78&title=black-hearts-yusufiyah-iraq.

Cheeseman, Peter. “In Defense of Probability.” In Proceedings of the 9th International Joint Conference on Artificial Intelligence - Volume 2, 1002–1009. IJCAI’85. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1985. http://dl.acm.org/citation.cfm?id=1623611.1623677.

“China’s Military Is Rushing to Use Artificial Intelligence - MIT Technology Review.” Accessed December 9, 2019. https://www.technologyreview.com/f/612915/chinas-military-is-rushing-to-use-artificial-intelligence/.

“China’s Surveillance State Should Scare Everyone - The Atlantic.” Accessed December 9, 2019. https://www.theatlantic.com/international/archive/2018/02/china-surveillance/552203/.

Dreyfus, Hubert L. “ALCHEMY AND ARTIFICIAL INTELLIGENCE,.” RAND CORP SANTA MONICA CALIF, December 1965. https://apps.dtic.mil/docs/citations/AD0625719.

The Strategist. “Ethical AI for Defence,” August 19, 2019. https://www.aspistrategist.org.au/ethical-ai-for-defence/.

Harbaugh, Jennifer. “Biography of Wernher Von Braun.” Text. NASA, February 18, 2016. http://www.nasa.gov/centers/marshall/history/vonbraun/bio.html.

“Has Global Violence Declined? A Look at the Data - Towards Data Science.” Accessed December 9, 2019. https://towardsdatascience.com/has-global-violence-declined-a-look-at-the-data-5af708f47fba.

“Hiroshima: The Harry Truman Diary and Papers.” Accessed December 9, 2019. http://www.doug-long.com/hst.htm.

“Laws, by Plato.” Accessed December 9, 2019. https://www.gutenberg.org/files/1750/1750-h/1750-h.htm.

“Mobile Vision  |  Google Developers.” Accessed December 9, 2019. https://developers.google.com/vision.

“North Korea Pursuing Domestic Development of AI, State Media Says - UPI.Com.” Accessed December 9, 2019. https://www.upi.com/Top_News/World-News/2019/07/31/North-Korea-pursuing-domestic-development-of-AI-state-media-says/3811564584016/.

“Pros and Cons of Autonomous Weapons Systems.” Accessed December 9, 2019. https://www.armyupress.army.mil/Journals/Military-Review/English-Edition-Archives/May-June-2017/Pros-and-Cons-of-Autonomous-Weapons-Systems/.

The Strategist. “Red Cross Is Seeking Rules for the Use of ‘Killer Robots,’” December 17, 2018. https://www.aspistrategist.org.au/red-cross-is-seeking-rules-for-the-use-of-killer-robots/.

“Russia Has Released Footage of Its New ‘Hunter’ Stealth Attack Drone - CNN.” Accessed December 9, 2019. https://www.cnn.com/2019/08/08/europe/russia-hunter-drone-scli-intl/index.html.

Atlantic Council. “Sanctions Propel Iran in the Global Race for Terminator-like AI,” April 2, 2019. https://www.atlanticcouncil.org/blogs/iransource/sanctions-propel-iran-in-the-global-race-for-terminator-like-ai/.

Spiegeleire, Stephan De, Matthijs Maas, and Tim Sweijs. Artificial Intelligence and the Future of Defense: Strategic Implications For Small- and Medium-Sized Force Providers. The Hague Centre for Strategic Studies, 2017.

“The First Drones, Used in World War I.” Accessed December 9, 2019. https://io9.gizmodo.com/the-first-drones-used-in-world-war-i-453365075.

“TRACE.” Accessed December 9, 2019. https://www.darpa.mil/program/trace.

“UAVs.” Accessed December 9, 2019. https://web.archive.org/web/20130601053222/http://www.airpower.maxwell.af.mil/airchronicles/cc/uav.html.

University, Carnegie Mellon. “The Balance of AI, Ethics and the Military - News - Carnegie Mellon University,” $dateFormat. http://www.cmu.edu/news/stories/archives/2019/march/ai-ethics.html.

University, Stanford. “Ethics of Autonomous Weapons.” Stanford News, May 1, 2019. https://news.stanford.edu/2019/05/01/ethics-autonomous-weapons/.

Waldrop, M. M. “Man-Made Minds: The Promise of Artificial Intelligence,” January 1, 1987. https://www.osti.gov/biblio/6449864.

“Weapons of the Weak: Russia and AI-Driven Asymmetric Warfare.” Accessed December 9, 2019. https://www.brookings.edu/research/weapons-of-the-weak-russia-and-ai-driven-asymmetric-warfare/.

“ارتش رباتیک ایران فراتر از مرزها - Sputnik Iran.” Accessed December 9, 2019. https://ir.sputniknews.com/opinion/201902044411288-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86-%D8%A7%D8%B1%D8%AA%D8%B4-%D8%B1%D8%A8%D8%A7%D8%AA%DB%8C%DA%A9/.